import tensorflow as tf
from tensorflow.keras import layers, models


num_words = 10000
max_len = 200

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=num_words)

x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)
x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_len)


model = models.Sequential([
    layers.Embedding(input_dim=num_words, output_dim=128, input_length=max_len),


    layers.GlobalAveragePooling1D(),

    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),

    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),

    layers.Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    x_train, y_train,
    epochs=5,
    batch_size=64,
    validation_split=0.2
)

loss, acc = model.evaluate(x_test, y_test)
print("Test Accuracy:", acc)


word_index = tf.keras.datasets.imdb.get_word_index()

word_index = {k: (v + 3) for k, v in word_index.items()}
word_index["<PAD>"] = 0
word_index["<START>"] = 1
word_index["<UNK>"] = 2
word_index["<UNUSED>"] = 3


reverse_index = {value: key for key, value in word_index.items()}
def encode_review(text):
    words = text.lower().split()
    encoded = [1]  # 1 = <START>

    for word in words:
        if word in word_index:
            encoded.append(word_index[word])
        else:
            encoded.append(2)  # <UNK>

    return tf.keras.preprocessing.sequence.pad_sequences([encoded], maxlen=200)
def predict_review(text):
    encoded = encode_review(text)
    pred = model.predict(encoded)[0][0]

    if pred >= 0.5:
        sentiment = "POSITIVE ðŸ˜Š"
    else:
        sentiment = "NEGATIVE ðŸ˜¡"

    print(f"Review: {text}")
    print(f"Prediction Score: {pred:.4f}")
    print(f"Sentiment: {sentiment}")
predict_review("This movie was absolutely wonderful, I loved it so much!")
predict_review("Worst movie ever. Waste of time.")


model.summary()
